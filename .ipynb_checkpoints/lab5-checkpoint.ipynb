{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab5 - Conditional Sequence-to-sequence VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange\n",
    "from nltk.translate.bleu_score import SmoothingFunction, sentence_bleu\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute BLEU-4 score\n",
    "def compute_bleu(output, reference):\n",
    "    cc = SmoothingFunction()\n",
    "    if len(reference) == 3:\n",
    "        weights = (0.33, 0.33, 0.33)\n",
    "    else:\n",
    "        weights = (0.25, 0.25, 0.25, 0.25)\n",
    "    return sentence_bleu([reference], output, weights=weights, smoothing_function=cc.method1)\n",
    "\n",
    "# Compute Gaussian score\n",
    "def Gaussian_score(words):\n",
    "    words_list = []\n",
    "    score = 0\n",
    "    yourpath = 'data/train.txt'#should be your directory of train.txt\n",
    "    with open(yourpath,'r') as fp:\n",
    "        for line in fp:\n",
    "            word = line.split(' ')\n",
    "            word[3] = word[3].strip('\\n')\n",
    "            words_list.extend([word])\n",
    "        for t in words:\n",
    "            for i in words_list:\n",
    "                if t == i:\n",
    "                    score += 1\n",
    "    return score/len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define characteristic to vector dictionary for embedding\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "ch_to_ix = {ch: i+2 for i, ch in enumerate(string.ascii_lowercase)}\n",
    "ix_to_ch = {i+2: ch for i, ch in enumerate(string.ascii_lowercase)}\n",
    "\n",
    "# Define condition to dictionary\n",
    "conditions = ['sp', 'tp', 'pg', 'p']\n",
    "cond_to_ix = {conditions[i]: i for i in range(len(conditions))}\n",
    "\n",
    "# Load the datasets\n",
    "def load_data(filename):\n",
    "    data = []\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            data.append(line.rstrip().split(' '))\n",
    "    return data\n",
    "\n",
    "\n",
    "train_data = load_data('./data/train.txt')\n",
    "train_cond = ['sp', 'tp', 'pg', 'p']\n",
    "\n",
    "test_data = load_data('./data/test.txt')\n",
    "test_cond = [\n",
    "    ['sp', 'p'],\n",
    "    ['sp', 'pg'],\n",
    "    ['sp', 'tp'],\n",
    "    ['sp', 'tp'],\n",
    "    ['p', 'tp'],\n",
    "    ['sp', 'pg'],\n",
    "    ['p', 'sp'],\n",
    "    ['pg', 'sp'],\n",
    "    ['pg', 'p'],\n",
    "    ['pg', 'tp'],\n",
    "] # Given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, latent_size, num_conds, cond_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.latent_size = latent_size\n",
    "        self.cond_size = cond_size\n",
    "        \n",
    "        # Embedding the input\n",
    "        self.word_embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.cond_embedding = nn.Embedding(num_conds, cond_size)\n",
    "        \n",
    "        # LSTM\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size)\n",
    "        \n",
    "        # Fully connected layer to generate mean and var for hidden and cell\n",
    "        self.hidden_mean = nn.Linear(hidden_size, latent_size)\n",
    "        self.hidden_logvar = nn.Linear(hidden_size, latent_size)\n",
    "        self.cell_mean = nn.Linear(hidden_size, latent_size)\n",
    "        self.cell_logvar = nn.Linear(hidden_size, latent_size)\n",
    "        \n",
    "    def forward(self, input, condition, hidden):\n",
    "        condition = self.cond_embedding(condition).view(1, 1, -1)\n",
    "        hidden = (torch.cat((hidden[0], condition), dim=2), torch.cat((hidden[1], condition), dim=2))\n",
    "        output = self.word_embedding(input).view(input.size(0), 1, -1)\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        \n",
    "        mean = (self.hidden_mean(hidden[0]), self.cell_mean(hidden[1]))\n",
    "        logvar = (self.hidden_logvar(hidden[0]), self.cell_logvar(hidden[1]))\n",
    "        \n",
    "        # Reparameterize\n",
    "        std = (torch.exp(logvar[0]), torch.exp(logvar[1]))\n",
    "        eps = (torch.rand_like(std[0]), torch.rand_like(std[1]))\n",
    "        latent = (mean[0]+eps[0]*std[0], mean[1]+eps[1]*std[1])\n",
    "        \n",
    "        return latent, mean, logvar\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        return (torch.zeros(1, 1, self.hidden_size-self.cond_size, device=device),\n",
    "                torch.zeros(1, 1, self.hidden_size-self.cond_size, device=device))\n",
    "    \n",
    "    def condition_embedding(self, condition):\n",
    "        return self.cond_embedding(condition)\n",
    "    \n",
    "    \n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        \n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size)\n",
    "        \n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return (torch.zeros(1, 1, self.hidden_size, device=device),\n",
    "                torch.zeros(1, 1, self.hidden_size, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(decoder, hidden, target_tensor, teacher_forcing):\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "    decoder_hidden = hidden\n",
    "    \n",
    "    target_length = target_tensor.size(0)\n",
    "    \n",
    "    outputs = []\n",
    "    if teacher_forcing:\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            outputs.append(decoder_output)\n",
    "            decoder_input = target_tensor[di]\n",
    "    else:\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            outputs.append(decoder_output)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "            \n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "                \n",
    "    outputs = torch.cat(outputs, dim=0)\n",
    "    return outputs\n",
    "    \n",
    "\n",
    "def train_pair(input_data_tensor, input_cond_tensor, target_data_tensor, target_cond_tensor,\n",
    "               encoder, encoder_optimizer, decoder, decoder_optimizer, kl_weight,\n",
    "               criterion, teacher_forcing_ratio=0.5, max_length=MAX_LENGTH):\n",
    "    \n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    input_length = input_data_tensor.size(0)\n",
    "    target_length = target_data_tensor.size(0)\n",
    "    \n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    #----------sequence to sequence part for encoder----------#\n",
    "    latent, mean, logvar = encoder(input_data_tensor, input_cond_tensor, encoder_hidden)\n",
    "    \n",
    "    #----------sequence to sequence part for decoder----------#\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "    \n",
    "    decoder_cond = encoder.condition_embedding(target_cond_tensor).view(1, 1, -1)\n",
    "    decoder_hidden = (torch.cat((latent[0], decoder_cond), dim=2), torch.cat((latent[1], decoder_cond), dim=2))\n",
    "    output = decode(decoder, decoder_hidden, target_data_tensor, teacher_forcing=use_teacher_forcing)\n",
    "    \n",
    "    crossEntropy_loss = criterion(output, target_data_tensor[:output.size(0)].view(-1))\n",
    "    kl_loss = torch.sum(0.5*(-logvar[0]+(mean[0]**2)+torch.exp(logvar[0])-1)) + torch.sum(0.5*(-logvar[1]+(mean[1]**2)+torch.exp(logvar[1])-1))\n",
    "    (crossEntropy_loss+kl_weight*kl_loss).backward()\n",
    "    \n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return crossEntropy_loss.item(), kl_loss.item()\n",
    "    \n",
    "    \n",
    "def predict(input_data_tensor, input_cond_tensor, target_data_tensor, target_cond_tensor, \n",
    "            encoder, decoder, criterion, max_length=MAX_LENGTH):\n",
    "    \n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    input_data_length = input_data_tensor.size(0)\n",
    "    \n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        latent, mean, logvar = encoder(input_data_tensor, input_cond_tensor, encoder_hidden)\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "        decoder_cond = encoder.cond_embedding(target_cond_tensor).view(1, 1, -1)\n",
    "        decoder_hidden = (torch.cat((latent[0], decoder_cond), dim=2), torch.cat((latent[1], decoder_cond), dim=2))\n",
    "        \n",
    "        crossEntropy_loss = 0\n",
    "        kl_loss = torch.sum(0.5*(-logvar[0]+(mean[0]**2)+torch.exp(logvar[0])-1)) + torch.sum(0.5*(-logvar[1]+(mean[1]**2)+torch.exp(logvar[1])-1))\n",
    "        \n",
    "        pred = ''\n",
    "        for di in range(target_data_tensor.size(0)):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "            \n",
    "            crossEntropy_loss += criterion(decoder_output, target_data_tensor[di])\n",
    "            if decoder_input == EOS_token:\n",
    "                break\n",
    "            \n",
    "            pred += ix_to_ch[decoder_input.item()]\n",
    "    \n",
    "    return pred, crossEntropy_loss, kl_loss\n",
    "    \n",
    "\n",
    "def train_data_combination(n=4):\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            yield (i, j)\n",
    "            yield (j, i)\n",
    "            \n",
    "\n",
    "def get_kl_anealing_func(method, slope=0.0002, max_val=1, period=None):\n",
    "    if method == 'monotonic':\n",
    "        return lambda iteration: min(iteration*slope, max_val)\n",
    "    elif method == 'cyclical' and period:\n",
    "        return lambda iteration: min((iteration%period)*slope, max_val)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "\n",
    "def train(encoder, decoder, train_dataset, train_condition, test_dataset, test_condition,\n",
    "          kl_anealing_func, teacher_forcing_ratio, num_epochs=20, learning_rate=1e-02):\n",
    "    \n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    train_result_list = {'cross_entropy': [], 'kl': []}\n",
    "    test_result_list = {'cross_entropy': [], 'kl': [], 'score': []}\n",
    "    iteration = 0\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # Train\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "        \n",
    "        shuffled_idx = np.random.permutation(len(train_dataset))\n",
    "        train_loss = {'cross_entropy': 0, 'kl': 0}\n",
    "        for i in range(len(train_dataset)):\n",
    "            data = train_dataset[shuffled_idx[i]]\n",
    "            cond = train_condition\n",
    "            for a, b in train_data_combination(4):\n",
    "                input_data_tensor = torch.tensor([ch_to_ix[ch] for ch in data[a]], device=device).view(-1, 1)\n",
    "                input_cond_tensor = torch.tensor(cond_to_ix[train_cond[a]], device=device)\n",
    "                target_data_tensor = torch.tensor([ch_to_ix[ch] for ch in data[b]]+[EOS_token], device=device).view(-1, 1)\n",
    "                target_cond_tensor = torch.tensor(cond_to_ix[train_cond[b]], device=device)\n",
    "                loss = train_pair(input_data_tensor, input_cond_tensor, target_data_tensor, target_cond_tensor,\n",
    "                           encoder, encoder_optimizer, decoder, decoder_optimizer, kl_anealing_func(iteration),\n",
    "                           criterion, teacher_forcing_ratio)\n",
    "                train_loss['cross_entropy'] += loss[0]\n",
    "                train_loss['kl'] += loss[1]\n",
    "                \n",
    "        train_loss['cross_entropy'] /= (len(train_dataset)*12)\n",
    "        train_loss['kl'] /= (len(train_dataset)*12)\n",
    "        train_result_list['cross_entropy'].append(train_loss['cross_entropy'])\n",
    "        train_result_list['kl'].append(train_loss['kl'])\n",
    "                \n",
    "        # Test\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "        \n",
    "        test_loss = {'cross_entropy': 0, 'kl': 0}\n",
    "        test_score = 0\n",
    "        for i in range(len(test_dataset)):\n",
    "            input_data_tensor = torch.tensor([ch_to_ix[ch] for ch in test_dataset[i][0]], device=device).view(-1, 1)\n",
    "            input_cond_tensor = torch.tensor(cond_to_ix[test_condition[i][0]], device=device)\n",
    "            target_data_tensor = torch.tensor([ch_to_ix[ch] for ch in test_dataset[i][1]], device=device).view(-1, 1)\n",
    "            target_cond_tensor = torch.tensor(cond_to_ix[test_condition[i][1]], device=device)\n",
    "            pred, crossEntropy_loss, kl_loss = predict(input_data_tensor, input_cond_tensor, target_data_tensor, target_cond_tensor, encoder, decoder, criterion)\n",
    "            test_loss['cross_entropy'] += crossEntropy_loss\n",
    "            test_loss['kl'] += kl_loss\n",
    "            score = compute_bleu(pred, test_dataset[i][1])\n",
    "            test_score += score\n",
    "        \n",
    "        test_loss['cross_entropy'] /= len(test_dataset)\n",
    "        test_loss['kl'] /= len(test_dataset)\n",
    "        test_result_list['cross_entropy'].append(test_loss['cross_entropy'])\n",
    "        test_result_list['kl'].append(test_loss['kl'])\n",
    "        test_score /= len(test_dataset)\n",
    "        test_result_list['score'].append(test_score)\n",
    "        \n",
    "        # Print the result\n",
    "        print('=====================================================')\n",
    "        print('Epoch: {} / {}'.format(epoch+1, num_epochs))\n",
    "        print()\n",
    "        print('Train Cross Entropy Loss: {}'.format(train_loss['cross_entropy']))\n",
    "        print('Train KL Loss: {}'.format(train_loss['kl']))\n",
    "        print()\n",
    "        print('Test Cross Entropy Loss: {}'.format(test_loss['cross_entropy']))\n",
    "        print('Test KL Loss: {}'.format(test_loss['kl']))\n",
    "        print('Test BLEU-4 Score: {}'.format(test_score))\n",
    "        print()\n",
    "        print()\n",
    "         \n",
    "    return encoder, decoder, train_result_list, test_result_list\n",
    "\n",
    "\n",
    "def evaluate(test_dataset, test_condition, encoder, decoder, max_length=MAX_LENGTH):\n",
    "    \n",
    "    score = 0\n",
    "    for i in range(len(test_dataset)):\n",
    "        input_data_tensor = torch.tensor([ch_to_ix[ch] for ch in test_dataset[i][0]], device=device).view(-1, 1)\n",
    "        input_cond_tensor = torch.tensor(cond_to_ix[test_condition[i][0]], device=device)\n",
    "        target_cond_tensor = torch.tensor(cond_to_ix[test_condition[i][1]], device=device)\n",
    "            \n",
    "        encoder_hidden = encoder.init_hidden()\n",
    "        input_data_length = input_data_tensor.size(0)\n",
    "    \n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "    \n",
    "        with torch.no_grad():\n",
    "        \n",
    "            latent, mean, logvar = encoder(input_data_tensor, input_cond_tensor, encoder_hidden)\n",
    "            decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "            decoder_cond = encoder.cond_embedding(target_cond_tensor).view(1, 1, -1)\n",
    "            decoder_hidden = (torch.cat((latent[0], decoder_cond), dim=2), torch.cat((latent[1], decoder_cond), dim=2))\n",
    "\n",
    "        \n",
    "            pred = ''\n",
    "            while True:\n",
    "                decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "                topv, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze().detach()\n",
    "\n",
    "                if decoder_input == EOS_token:\n",
    "                    break\n",
    "            \n",
    "                pred += ix_to_ch[decoder_input.item()]\n",
    "    \n",
    "        score += compute_bleu(pred, test_dataset[i][1])\n",
    "        \n",
    "        print('Input:      {}'.format(test_dataset[i][0]))\n",
    "        print('Target:     {}'.format(test_dataset[i][1]))\n",
    "        print('Prediction: {}'.format(pred))\n",
    "        print()\n",
    "    print('Average BLEU-4 Score: {}'.format(score/len(test_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "Epoch: 1 / 5\n",
      "\n",
      "Train Cross Entropy Loss: 0.0011314563325108148\n",
      "Train KL Loss: 1107148.3614365729\n",
      "\n",
      "Test Cross Entropy Loss: 0.008216426707804203\n",
      "Test KL Loss: 1031.5662841796875\n",
      "Test BLEU-4 Score: 1.0\n",
      "\n",
      "\n",
      "=====================================================\n",
      "Epoch: 2 / 5\n",
      "\n",
      "Train Cross Entropy Loss: 0.0010978779801758902\n",
      "Train KL Loss: 1112188.0331284632\n",
      "\n",
      "Test Cross Entropy Loss: 0.007876241579651833\n",
      "Test KL Loss: 1029.3585205078125\n",
      "Test BLEU-4 Score: 1.0\n",
      "\n",
      "\n",
      "=====================================================\n",
      "Epoch: 3 / 5\n",
      "\n",
      "Train Cross Entropy Loss: 0.0010828311358603232\n",
      "Train KL Loss: 895540.7413091745\n",
      "\n",
      "Test Cross Entropy Loss: 0.00800984725356102\n",
      "Test KL Loss: 1026.9705810546875\n",
      "Test BLEU-4 Score: 1.0\n",
      "\n",
      "\n",
      "=====================================================\n",
      "Epoch: 4 / 5\n",
      "\n",
      "Train Cross Entropy Loss: 0.0010611051219330929\n",
      "Train KL Loss: 913191.0349916626\n",
      "\n",
      "Test Cross Entropy Loss: 0.007418683264404535\n",
      "Test KL Loss: 1034.4884033203125\n",
      "Test BLEU-4 Score: 1.0\n",
      "\n",
      "\n",
      "=====================================================\n",
      "Epoch: 5 / 5\n",
      "\n",
      "Train Cross Entropy Loss: 0.001034282368021017\n",
      "Train KL Loss: 892065.9049988104\n",
      "\n",
      "Test Cross Entropy Loss: 0.007953687570989132\n",
      "Test KL Loss: 1031.2242431640625\n",
      "Test BLEU-4 Score: 1.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameters\n",
    "hidden_size = 512\n",
    "latent_size = 64\n",
    "vocab_size = 28\n",
    "num_conds = 4\n",
    "cond_size = 32\n",
    "teacher_forcing_ratio = 0.5\n",
    "learning_rate = 0.001\n",
    "MAX_LENGTH = 40\n",
    "num_epochs = 5\n",
    "\n",
    "# Encoder & Decoder\n",
    "#encoder = EncoderRNN(vocab_size, hidden_size, latent_size, num_conds, cond_size).to(device)\n",
    "#decoder = DecoderRNN(latent_size+cond_size, vocab_size).to(device)\n",
    "\n",
    "# Train\n",
    "encoder, decoder, train_loss_list, test_score_list = train(encoder, decoder,\n",
    "                                                           train_data, train_cond, test_data, test_cond,\n",
    "                                                           kl_anealing_func=get_kl_anealing_func('cyclical', period=10000),\n",
    "                                                           teacher_forcing_ratio=teacher_forcing_ratio,\n",
    "                                                           num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_words(encoder, decoder, num=100):\n",
    "    \n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    cond_size = encoder.cond_size\n",
    "    embedded_conditions = {}\n",
    "    for cond in conditions:\n",
    "        cond_tensor = torch.tensor(cond_to_ix[cond], device=device)\n",
    "        embedded_conditions[cond] = encoder.condition_embedding(cond_tensor)\n",
    "    \n",
    "    words_list = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num):\n",
    "            noise = decoder.init_hidden()\n",
    "\n",
    "            # Generate Gaussian noise\n",
    "            noise = (noise[0].normal_(std=1), noise[1].normal_(std=1))\n",
    "\n",
    "            # Generate words with 4 different tenses\n",
    "            words = []\n",
    "            for embedded_cond in embedded_conditions.values():\n",
    "                \n",
    "                noise[0][:, :, -cond_size:] = embedded_cond\n",
    "                noise[1][:, :, -cond_size:] = embedded_cond\n",
    "                \n",
    "                decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "                decoder_hidden = noise\n",
    "            \n",
    "                word = ''\n",
    "                while True:\n",
    "                    decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "                    topv, topi = decoder_output.topk(1)\n",
    "                    decoder_input = topi.squeeze().detach()\n",
    "                    \n",
    "                    if decoder_input == EOS_token:\n",
    "                        break\n",
    "                        \n",
    "                    word += ix_to_ch[decoder_input.item()]\n",
    "                words.append(word)\n",
    "            words_list.append(words)\n",
    "    return words_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
